{
    "meta_model_params" : {
        "input_size": 512,
        "hidden_dim": 512,
        "num_layers": 1,
        "dropout": 0.0
    },
    "word_model_params": {
        "hidden_dim": 256,
        "mlp_proj_size": 256,
        "num_layers": 1,
        "dropout": 0.0,
        "pretrained_embs_path": "data/embeddings/model.txt"
    },
    "char_model_params": {
        "emb_dim": 100,
        "hidden_dim": 256,
        "mlp_proj_size": 256,
        "num_layers": 1,
        "dropout": 0.0
    }
}